{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model and XGBOOST Model of the Abalone Dataset\n",
    "## Import Modules and Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "\n",
    "# load in datas\n",
    "df_train = pd.read_csv('/home/tomruge/Schreibtisch/Data/Kaggle/playground-series-s4e4/train.csv')\n",
    "df_test = pd.read_csv('/home/tomruge/Schreibtisch/Data/Kaggle/playground-series-s4e4/test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple one hot encode\n",
    "df_train = pd.get_dummies(df_train)\n",
    "df_test = pd.get_dummies(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 90615 entries, 0 to 90614\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   id              90615 non-null  int64  \n",
      " 1   Length          90615 non-null  float64\n",
      " 2   Diameter        90615 non-null  float64\n",
      " 3   Height          90615 non-null  float64\n",
      " 4   Whole weight    90615 non-null  float64\n",
      " 5   Whole weight.1  90615 non-null  float64\n",
      " 6   Whole weight.2  90615 non-null  float64\n",
      " 7   Shell weight    90615 non-null  float64\n",
      " 8   Rings           90615 non-null  int64  \n",
      " 9   Sex_F           90615 non-null  bool   \n",
      " 10  Sex_I           90615 non-null  bool   \n",
      " 11  Sex_M           90615 non-null  bool   \n",
      "dtypes: bool(3), float64(7), int64(2)\n",
      "memory usage: 6.5 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class model_class:\n",
    "    def __init__(self, df_train, df_test, target = [], drop = []):\n",
    "        self.df_train = df_train.drop(columns = drop)\n",
    "        self.df_test = df_test.drop(columns = drop)\n",
    "        self.target = target\n",
    "        self.drop = drop\n",
    "\n",
    "    def feature_engineering(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self,params):\n",
    "        prob_scores = []  \n",
    "        for i in range(1):\n",
    "            # the i iteration does not have a function yet, will be implemented later\n",
    "            n_splits = 5\n",
    "            mskf = StratifiedShuffleSplit(n_splits=n_splits, random_state=i)\n",
    "            result = np.zeros((self.df_test.shape[0], (i+1)*n_splits))\n",
    "            for itteration, (train_index, test_index) in enumerate(mskf.split(self.df_train, self.df_train[self.target])):\n",
    "                X_train = self.df_train.loc[train_index].drop(columns = self.target)\n",
    "                X_test = self.df_train.loc[test_index].drop(columns = self.target)\n",
    "                y_train = self.df_train.loc[train_index][self.target]\n",
    "                y_test = self.df_train.loc[test_index][self.target]\n",
    "                model = XGBRegressor(**params)\n",
    "                # make the value sof the x and y float 64\n",
    "                X_train = X_train.astype('float64')\n",
    "                y_train = y_train.astype('float64')\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(self.df_test)\n",
    "                result[:, (i+1)*itteration] = y_pred\n",
    "\n",
    "        return np.mean(result, axis=1)\n",
    "    \n",
    "    def root_mean_squared_log_error(self, y_true, y_pred):\n",
    "        return np.sqrt(np.mean(np.square(np.log1p(y_true) - np.log1p(y_pred))))\n",
    "    \n",
    "    def objective(self,trial):\n",
    "        # xgboost regressor parameters\n",
    "        xgbc_params = {}\n",
    "        prediction = self.fit(xgbc_params)\n",
    "        score = self.root_mean_squared_log_error(self.df_train[self.target], prediction)\n",
    "        return score\n",
    "    \n",
    "    def predict(self, params):\n",
    "        return self.fit(params)\n",
    "    \n",
    "    def find_params(self):\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(self.objective, n_trials=10, n_jobs=7)\n",
    "        return study.best_params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "target_columns = ['Rings']\n",
    "drop_columns = ['id']\n",
    "model = model_class(df_train, df_test, target_columns, drop_columns)\n",
    "# it was 445, i rduce to 50\n",
    "xgb_params = {\n",
    "            'n_estimators': 500,\n",
    "            'max_depth': 6,\n",
    "            'learning_rate': 0.0116,\n",
    "            'colsample_bytree': 1,\n",
    "            'subsample': 0.6085,\n",
    "            'min_child_weight': 9,\n",
    "            'reg_lambda': 4.879e-07,\n",
    "            'max_bin': 431,\n",
    "            'n_jobs': -1,\n",
    "            'eval_metric': 'mae',\n",
    "            'objective': \"reg:absoluteerror\",\n",
    "            'tree_method': 'hist',\n",
    "            'verbosity': 0,\n",
    "            'random_state': 42,\n",
    "        }\n",
    "\n",
    "# make predictions and save them\n",
    "prediction = model.fit(params=xgb_params)\n",
    "predictions = pd.DataFrame(np.column_stack((df_test['id'].astype('Int32'), prediction)), columns = ['id', 'Rings'])\n",
    "\n",
    "# save predictions\n",
    "predictions.to_csv('/home/tomruge/Schreibtisch/Data/Kaggle/playground-series-s4e4/v1_simple_xgboost.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This performed a little worse then my simple xgboost. So iterating over subsets and combining them does not improve the integrated ROC corve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
