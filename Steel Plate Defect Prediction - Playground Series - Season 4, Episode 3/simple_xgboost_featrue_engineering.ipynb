{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load in data\n",
    "df_train = pd.read_csv('/home/tomruge/Schreibtisch/Data/Kaggle/playground-series-s4e3/train.csv')\n",
    "df_test = pd.read_csv('/home/tomruge/Schreibtisch/Data/Kaggle/playground-series-s4e3/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomruge/anaconda3/envs/env3.12.2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from xgboost import XGBClassifier\n",
    "import optuna\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class model_class:\n",
    "    def __init__(self, df_train, df_test, target = [], drop = []):\n",
    "        self.df_train = df_train.drop(columns = drop)\n",
    "        self.df_test = df_test.drop(columns = drop)\n",
    "        self.target = target\n",
    "        self.drop = drop\n",
    "        # preprocessing\n",
    "        self.df_train = self.preprocess_data(self.df_train)\n",
    "        self.df_test = self.preprocess_data(self.df_test)\n",
    "\n",
    "    def preprocess_data(self,data):   \n",
    "        # taken from hre: https://www.kaggle.com/competitions/playground-series-s4e3/discussion/482475\n",
    "        data['_Aspect_Ratio'] = (data['X_Maximum'] - data['X_Minimum']) / (abs(data['Y_Maximum'] - data['Y_Minimum']) + 1)\n",
    "        data['_Area_Perimeter_Ratio_X'] = data['Pixels_Areas'] / data['X_Perimeter']\n",
    "        data['_Area_Perimeter_Ratio_Y'] = data['Pixels_Areas'] / data['Y_Perimeter']\n",
    "        data['_Compactness_X'] = data['Pixels_Areas'] / (data['X_Perimeter'] ** 2)\n",
    "        data['_Compactness_Y'] = data['Pixels_Areas'] / (data['Y_Perimeter'] ** 2)\n",
    "        data['_Color_Range'] = data['Maximum_of_Luminosity'] - data['Minimum_of_Luminosity']\n",
    "        data['_Spatial_Distribution_Index'] = (\n",
    "            data['Edges_Index'] + data['Empty_Index'] + data['Square_Index'] + data['Outside_X_Index'] + data['Edges_X_Index'] + \n",
    "            data['Edges_Y_Index'] + data['Outside_Global_Index']\n",
    "        )\n",
    "        data['_Log_Area_Perimeter_Ratio'] = data['LogOfAreas'] / (data['Log_X_Index'] + data['Log_Y_Index'])\n",
    "        data['_Normalized_Luminosity_Index'] = data['Luminosity_Index'] / data['Pixels_Areas']\n",
    "        data['_Thickness_Steel_Type'] = data['Steel_Plate_Thickness'] * (data['TypeOfSteel_A300'] + data['TypeOfSteel_A400'])\n",
    "        data['_Edge_to_Area_Ratio'] = data['Edges_Index'] / data['Pixels_Areas']\n",
    "        \n",
    "        # taken from here: https://www.kaggle.com/competitions/playground-series-s4e3/discussion/481687\n",
    "        epsilon = 1e-6  # A small constant to avoid division by zero or taking the logarithm of zero\n",
    "        # Location Features\n",
    "        data['X_Distance'] = data['X_Maximum'] - data['X_Minimum']\n",
    "        data['Y_Distance'] = data['Y_Maximum'] - data['Y_Minimum']\n",
    "\n",
    "        # Density Feature\n",
    "        data['Density'] = data['Pixels_Areas'] / (data['X_Perimeter'] + data['Y_Perimeter'])\n",
    "\n",
    "        # Relative Perimeter Feature\n",
    "        data['Relative_Perimeter'] = data['X_Perimeter'] / (data['X_Perimeter'] + data['Y_Perimeter'] + epsilon)\n",
    "\n",
    "        # Circularity Feature\n",
    "        data['Circularity'] = data['Pixels_Areas'] / (data['X_Perimeter'] ** 2)\n",
    "\n",
    "        # Symmetry Index Feature\n",
    "        data['Symmetry_Index'] = np.abs(data['X_Distance'] - data['Y_Distance']) / (data['X_Distance'] + data['Y_Distance'] + epsilon)\n",
    "\n",
    "        # Color Contrast Feature\n",
    "        data['Color_Contrast'] = data['Maximum_of_Luminosity'] - data['Minimum_of_Luminosity']\n",
    "\n",
    "        # Combined Geometric Index Feature\n",
    "        data['Combined_Geometric_Index'] = data['Edges_Index'] * data['Square_Index']\n",
    "\n",
    "        # Interaction Term Feature\n",
    "        data['X_Distance*Pixels_Areas'] = data['X_Distance'] * data['Pixels_Areas']\n",
    "\n",
    "        # Additional Features\n",
    "        data['sin_orientation'] = np.sin(data['Orientation_Index'])\n",
    "        data['Edges_Index2'] = np.exp(data['Edges_Index'] + epsilon)\n",
    "        data['X_Maximum2'] = np.sin(data['X_Maximum'])\n",
    "        data['Y_Minimum2'] = np.sin(data['Y_Minimum'])\n",
    "        data['Aspect_Ratio_Pixels'] = np.where(data['Y_Perimeter'] == 0, 0, data['X_Perimeter'] / data['Y_Perimeter'])\n",
    "        data['Aspect_Ratio'] = np.where(data['Y_Distance'] == 0, 0, data['X_Distance'] / data['Y_Distance'])\n",
    "\n",
    "        # Average Luminosity Feature\n",
    "        data['Average_Luminosity'] = (data['Sum_of_Luminosity'] + data['Minimum_of_Luminosity']) / 2\n",
    "\n",
    "        # Normalized Steel Thickness Feature\n",
    "        data['Normalized_Steel_Thickness'] = (data['Steel_Plate_Thickness'] - data['Steel_Plate_Thickness'].min()) / (data['Steel_Plate_Thickness'].max() - data['Steel_Plate_Thickness'].min())\n",
    "\n",
    "        # Logarithmic Features\n",
    "        data['Log_Perimeter'] = np.log(data['X_Perimeter'] + data['Y_Perimeter'] + epsilon)\n",
    "        data['Log_Luminosity'] = np.log(data['Sum_of_Luminosity'] + epsilon)\n",
    "        data['Log_Aspect_Ratio'] = np.log(data['Aspect_Ratio'] ** 2 + epsilon)\n",
    "\n",
    "        # Statistical Features\n",
    "        data['Combined_Index'] = data['Orientation_Index'] * data['Luminosity_Index']\n",
    "        data['Sigmoid_Areas'] = 1 / (1 + np.exp(-data['LogOfAreas'] + epsilon))\n",
    "        return data\n",
    "    \n",
    "    def get_k_most_important_features(self, k):\n",
    "        pass\n",
    "\n",
    "    def fit(self,params, fit_not_evaluate = True):\n",
    "        roc_scores = []\n",
    "        mskf = MultilabelStratifiedKFold(n_splits=2, random_state=0, shuffle=True)\n",
    "        result = np.zeros((self.df_train.shape[0], len(self.target)))\n",
    "        for itteration, (train_index, test_index) in enumerate(mskf.split(self.df_train, self.df_train[self.target])):\n",
    "            X_train = self.df_train.loc[train_index].drop(columns = self.target)\n",
    "            X_test = self.df_train.loc[test_index].drop(columns = self.target)\n",
    "            y_train = self.df_train.loc[train_index][self.target]\n",
    "            y_test = self.df_train.loc[test_index][self.target]\n",
    "            model = XGBClassifier(**params, n_jobs = 3, random_state = itteration)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict_proba(X_test)\n",
    "            # i think this wont work, because with n_splits = 2, the validation set  will sty zero\n",
    "            result[test_index] = y_pred\n",
    "            roc_scores.append(roc_auc_score(y_test, y_pred))\n",
    "        if fit_not_evaluate:\n",
    "            return result\n",
    "        else:\n",
    "            return np.mean(roc_scores)\n",
    "    \n",
    "    def predict(self, params):\n",
    "        # fit to all the data\n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(self.df_train.drop(columns = self.target), self.df_train[self.target])\n",
    "        prediction =  model.predict_proba(self.df_test)\n",
    "        # plot feature importance for prediction in a bar chart with matplotlib\n",
    "        #print('IMportances:', pd.Series(model.feature_importances_, index = self.df_train.drop(columns = self.target).columns).sort_values(ascending = False).plot(kind = 'bar'))\n",
    "        return prediction\n",
    "    \n",
    "    def objective(self,trial):\n",
    "        # params for optimizing the xgboostclassifier,makke sure to use compatible ones\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.5, 1),\n",
    "            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1),\n",
    "            'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n",
    "            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),\n",
    "            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n",
    "            'use_label_encoder': False,\n",
    "            'eval_metric': 'logloss'\n",
    "        }\n",
    "        score = self.fit(params)\n",
    "        return score\n",
    "     \n",
    "    def find_params(self):\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(self.objective, n_trials=400, n_jobs=7)\n",
    "        return study.best_params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8823579743906221\n"
     ]
    }
   ],
   "source": [
    "# find the best params\n",
    "target_columns = ['Pastry','Z_Scratch','K_Scatch','Stains','Dirtiness','Bumps','Other_Faults']\n",
    "drop_columns = ['id']\n",
    "# these parameters propably apply to thedataset without feature engineering\n",
    "params2 = { 'n_estimators':1800,\n",
    "            'learning_rate': 0.006,\n",
    "            'gamma': 0.44,\n",
    "            'subsample': 0.7,\n",
    "            'colsample_bytree': 0.38,\n",
    "            'max_depth': 5,\n",
    "            'min_child_weight': 4,\n",
    "            'reg_lambda': 1.8e-06,\n",
    "            'reg_alpha': 0.54,\n",
    "            'booster':'gbtree',\n",
    "           'grow_policy': 'depthwise',\n",
    "            'verbosity': 0 ,#'device_type': 'cuda','tree_method': 'gpu_hist',}\n",
    "          }\n",
    "\n",
    "model = model_class(df_train, df_test, target_columns, drop_columns)\n",
    "roc = model.fit(params2, fit_not_evaluate = False)\n",
    "print(roc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> seems not to be that much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best params\n",
    "target_columns = ['Pastry','Z_Scratch','K_Scatch','Stains','Dirtiness','Bumps','Other_Faults']\n",
    "drop_columns = ['id']\n",
    "# these parameters propably apply to thedataset without feature engineering\n",
    "params2 = { 'n_estimators':1800,\n",
    "            'learning_rate': 0.006,\n",
    "            'gamma': 0.44,\n",
    "            'subsample': 0.7,\n",
    "            'colsample_bytree': 0.38,\n",
    "            'max_depth': 5,\n",
    "            'min_child_weight': 4,\n",
    "            'reg_lambda': 1.8e-06,\n",
    "            'reg_alpha': 0.54,\n",
    "            'booster':'gbtree',\n",
    "           'grow_policy': 'depthwise',\n",
    "            'verbosity': 0 ,#'device_type': 'cuda','tree_method': 'gpu_hist',}\n",
    "          }\n",
    "\n",
    "model = model_class(df_train, df_test, target_columns, drop_columns)\n",
    "roc_curve = model.predict(params = params2)\n",
    "\n",
    "predictions = pd.DataFrame(np.column_stack((df_test['id'].astype('Int32'), roc_curve)), columns = ['id', 'Pastry','Z_Scratch','K_Scatch','Stains','Dirtiness','Bumps','Other_Faults'])\n",
    "# save predictions\n",
    "predictions.to_csv('/home/tomruge/Schreibtisch/Data/Kaggle/playground-series-s4e3/predictions_simpledexgboost_1800_ca10_More Features.csv', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
